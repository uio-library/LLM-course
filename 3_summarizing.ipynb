{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1b0f5c9-8070-44b8-b1ea-c0a337d373ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Summarization\n",
    "\n",
    "We will use [LangChain](https://www.langchain.com/), an open-source library for making applications with LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d75d3df-17c0-4ff9-8659-36489f174b33",
   "metadata": {},
   "source": [
    "## Document location\n",
    "We will try to load  all the documents in the folder defined below.\n",
    "If you prefer, you can change this to a different folder name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e62dd1c-6122-45c1-90b3-16badbf11313",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#document_folder = 'documents'\n",
    "document_folder = '../summarizing'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8e02df-c8fc-4c67-8ba7-a4c96e2ee715",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "## Some configuration\n",
    "To conserve memory, we configure more efficient memory use on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fb9a33-487c-45b7-b443-eeec81a8c3ec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198a041d",
   "metadata": {},
   "source": [
    "## Installing Software\n",
    "Weâ€™ll need to install some libraries first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9908a60",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install unstructured[all-docs] langchain-unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62855d10-417b-4648-a14e-f5512c1f4f18",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## The Language Model\n",
    "We'll use models from [HuggingFace](https://huggingface.co/), a website that has tools and models for machine learning.\n",
    "We'll use the open-weights LLM \n",
    "[mistralai/Ministral-8B-Instruct-2410](https://huggingface.co/mistralai/Ministral-8B-Instruct-2410).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac38b664-7075-4ed6-9a21-bb1465b7c095",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "%env HF_HOME=/fp/projects01/ec443/huggingface/cache/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb595bd",
   "metadata": {},
   "source": [
    "To use the model, we create a *pipeline*.\n",
    "A pipeline can consist of several processing steps, but in this case, we only need one step.\n",
    "We can use the method `HuggingFacePipeline.from_model_id()`, which automatically downloads the specified model from HuggingFace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a647226e-d084-4066-b6f7-74c64f4764d2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "llm = pipeline(\"text-generation\", \n",
    "               model=\"mistralai/Mistral-Nemo-Instruct-2407\",\n",
    "               device=0,\n",
    "               max_new_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3059fb20-861e-4d7d-b5a4-cd141b8b352b",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    #model_id='mistralai/Mistral-Small-Instruct-2409',\n",
    "    model_id='mistralai/Ministral-8B-Instruct-2410',\n",
    "    #model_id='mistralai/Mistral-7B-Instruct-v0.3',\n",
    "    task='text-generation',\n",
    "    device=0,\n",
    "    pipeline_kwargs={\n",
    "        'max_new_tokens': 1000,\n",
    "        #'temperature': 0.3,\n",
    "        #'num_beams': 4,\n",
    "        #'do_sample': True\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42032173",
   "metadata": {},
   "source": [
    "\n",
    "We give some arguments to the pipeline:\n",
    "- `model_id`: the name of the  model on HuggingFace\n",
    "- `task`:  the task you want to use the model for\n",
    "- `device`: the GPU hardware device to use. If we don't specify a device, no GPU will be used.\n",
    "- `pipeline_kwargs`: additional parameters that are passed to the model.\n",
    "    - `max_new_tokens`: maximum length of the generated text\n",
    "    - `do_sample`: by default, the most likely next word is chosen.  This makes the output deterministic. We can introduce some randomness by sampling among the  most likely words instead.\n",
    "    - `temperature`: the temperature controls the amount of randomness, where zero means no randomness.\n",
    "    - `num_beams`: by default the model works with a single sequence of  tokens/words. With beam search, the program  builds multiple sequences at the same time, and then selects the best one in the end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e788a2",
   "metadata": {},
   "source": [
    "## Making a Prompt\n",
    "We can use a *prompt* to tell the language model how to answer.\n",
    "The prompt should contain a few short, helpful instructions.\n",
    "In addition, we provide placeholders for the context.\n",
    "LangChain replaces these with the actual documents when we execute a query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d166216-7da7-498a-bb58-73f9ecb451ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f7e173-efe0-4241-af09-a205ea6ba274",
   "metadata": {},
   "outputs": [],
   "source": [
    "separator = '\\nYour Summary:\\n'\n",
    "prompt_template = '''Write a summary of the following:\n",
    "\n",
    "{context}\n",
    "''' + separator\n",
    "prompt = PromptTemplate(template=prompt_template,\n",
    "                        input_variables=['context'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a9cdf9-4fa9-4e17-b20a-a1ef7f3445a3",
   "metadata": {},
   "source": [
    "## Create chain\n",
    "\n",
    "The document loader loads each PDF page as a separate 'document'.\n",
    "This is partly for technical reasons because that is the way PDFs are structured.\n",
    "Therefore, we use the chain called  `create_stuff_documents_chain` which joins multiple documents  into a single large document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4e44b3-2059-4a8c-860a-932d3ed1a467",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a917d7-bb6f-4e45-9f6d-2c6842598256",
   "metadata": {},
   "source": [
    "## Function to load and summarize a single document\n",
    "\n",
    "This function loads a single document and runs it through the language model to produce a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a13ba30-5729-4551-9bf6-b88927b3251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_result(result):\n",
    "    \"Split the reply from the prompt, should be done with output parser?\"\n",
    "    position = result.find(separator)\n",
    "    summary = result[position + len(separator) :]\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2970f78-337e-499d-abe8-d45f3ed4ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_unstructured import UnstructuredLoader\n",
    "\n",
    "def summarize_document(filename):\n",
    "    print('\\n\\nProcessing file:', filename)\n",
    "    loader = UnstructuredLoader(filename)\n",
    "    docs = loader.load()\n",
    "    document_lengths = [len(doc.page_content) for doc in docs]\n",
    "    print(f'Number of documents: {len(docs)}, total length: {sum(document_lengths)}')\n",
    "    print('Maximum document length: ', max(document_lengths))\n",
    "\n",
    "    #Run chain    \n",
    "    result = chain.invoke({\"context\": docs})\n",
    "    return split_result(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3fa297-2be5-499c-8e4f-d9777925002f",
   "metadata": {},
   "source": [
    "## Loading the Documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58d5773-652f-4fa1-86f7-8058b62f61dd",
   "metadata": {},
   "source": [
    "We use the Python library `pathlib` to iterate over all in files in `document_folder`.\n",
    "`document_folder` is defined at the start of this  Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c93ac6-1f9d-4cfb-a91d-606ff65930d0",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "directory = Path(document_folder)\n",
    "file_iterator = directory.iterdir()\n",
    "summaries  = dict()\n",
    "\n",
    "for filename in file_iterator:\n",
    "    try:\n",
    "         summary = summarize_document(filename)\n",
    "    except Exception as e:\n",
    "        print('Error:', e)\n",
    "    summaries[filename] = summary\n",
    "    #summaries.append(Document(page_content = summary))\n",
    "    \n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef69d3f-07cd-4d88-ba92-5d729d4f0e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('summaries.txt', 'w') as outfile:\n",
    "    for filename in summaries:\n",
    "        print('Summary of ', filename, file = outfile)\n",
    "        print(summaries[filename], file=outfile)\n",
    "        print(file=outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0121d8e7-9d91-43ac-b8cb-29f7c71f9865",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.schema.document import Document\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "total_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"Below is a list of summaries of some papers. Make a total summary all the information in all the papers:\\n\\n{context}\\n\\nTotal Summary:\")]\n",
    ")\n",
    "total_chain = create_stuff_documents_chain(llm, total_prompt)\n",
    "total_summary = total_chain.invoke({\"context\": [Document(page_content = summary) for summary in summaries.values()]})\n",
    "\n",
    "print('Summary of all the summaries:')\n",
    "print(total_summary)\n",
    "\n",
    "#print(result)\n",
    "\n",
    "with open('total_summary.txt', 'w') as outfile:\n",
    "    print(total_summary, file=outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
