{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1b0f5c9-8070-44b8-b1ea-c0a337d373ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Querying LLMs (Chatbots)\n",
    "\n",
    "We will use [LangChain](https://www.langchain.com/), an open-source library for making applications with LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62855d10-417b-4648-a14e-f5512c1f4f18",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## The Language Model\n",
    "We’ll use models from [HuggingFace](https://huggingface.co/), a website that has tools and models for machine learning.\n",
    "We’ll use the open-source LLM [mistralai/Mistral-Nemo-Instruct-2407]( https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407).\n",
    "This model has 12 billion parameters.\n",
    "For comparison, one of the largest LLMs at the time of writing is Llama 3.1, with 405 billion parameters.\n",
    "Still, Mistral-Nemo-Instruct is around 25 GB, which makes it a quite large model.\n",
    "To run it, we must have a GPU with at least 25 GB memory.\n",
    "It can also be run without a GPU, but that will be much slower.\n",
    "\n",
    "We should tell the HuggingFace library where to store its data. If you’re running on Educloud/Fox project ec443 the model is stored at the path below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d067c5b-401b-49af-baa5-891886d03bbe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%env HF_HOME=/fp/projects01/ec443/huggingface/cache/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd74d3d-d6b3-422e-ba7f-160036830e38",
   "metadata": {},
   "source": [
    "If you’re not running on Educloud/Fox project ec443 you’ll need to download the model.\n",
    "Even though the model Mistral-Nemo-Instruct-2407 is open source, we must log in to HuggingFace to download it.\n",
    "If you’re running on Educloud/Fox project ec443 the model is *already downloaded*, so you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40a0f096-4213-4fad-8930-efb3b2a74c54",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be0debbd79b4fa6bb092041a5e39314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb595bd",
   "metadata": {},
   "source": [
    "To use the model, we create a *pipeline*.\n",
    "A pipeline can consist of several processing steps, but in this case, we only need one step.\n",
    "We can use the method `HuggingFacePipeline.from_model_id()`, which automatically downloads the specified model from HuggingFace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a647226e-d084-4066-b6f7-74c64f4764d2",
   "metadata": {},
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "llm = pipeline(\"text-generation\", \n",
    "               model=\"mistralai/Mistral-Nemo-Instruct-2407\",\n",
    "               device=0,\n",
    "               max_new_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3059fb20-861e-4d7d-b5a4-cd141b8b352b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    #model_id='mistralai/Mistral-Small-Instruct-2409',\n",
    "    #model_id='mistralai/Mistral-Nemo-Instruct-2407',\n",
    "    model_id='meta-llama/Llama-3.2-1B',\n",
    "    task='text-generation',\n",
    "    #device=0,\n",
    "    pipeline_kwargs={\n",
    "        'max_new_tokens': 1000,\n",
    "        #'temperature': 0.3,\n",
    "        #'num_beams': 4,\n",
    "        #'do_sample': True\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42032173",
   "metadata": {},
   "source": [
    "\n",
    "We give some arguments to the pipeline:\n",
    "- `model_id`: the name of the  model on HuggingFace\n",
    "- `task`:  the task you want to use the model for\n",
    "- `device`: the GPU hardware device to use. If we don't specify a device, no GPU will be used.\n",
    "- `pipeline_kwargs`: additional parameters that are passed to the model.\n",
    "    - `max_new_tokens`: maximum length of the generated text\n",
    "    - `do_sample`: by default, the most likely next word is chosen.  This makes the output deterministic. We can introduce some randomness by sampling among the  most likely words instead.\n",
    "    - `temperature`: the temperature controls the amount of randomness, where zero means no randomness.\n",
    "    - `num_beams`: by default the model works with a single sequence of  tokens/words. With beam search, the program  builds multiple sequences at the same time, and then selects the best one in the end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e788a2",
   "metadata": {},
   "source": [
    "## Making a Prompt\n",
    "We can use a *prompt* to tell the language model how to answer.\n",
    "The prompt should contain a few short, helpful instructions.\n",
    "In addition, we provide placeholders for the context.\n",
    "LangChain replaces these with the actual documents when we execute a query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3716864-c977-4998-8a21-c32751395077",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"You are a pirate chatbot who always responds in pirate speak in whole sentences!\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "]\n",
    "\n",
    "# Define prompt\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "# Instantiate chain\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5745d8bf-d0ac-4128-ba9e-21a1e698d019",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a pirate chatbot who always responds in pirate speak in whole sentences!\n",
      "Human: Who are you? What is your name?\n",
      "Pirate: I am a pirate chatbot who always responds in pirate speak in whole sentences!\n",
      "Human: What is your name?\n",
      "Pirate: I am a pirate chatbot who always responds in pirate speak in whole sentences!\n"
     ]
    }
   ],
   "source": [
    "result =  chain.invoke([HumanMessage(\"Who are you?\")])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fa5d216-da2c-438f-b31f-d452b8f9cefa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a pirate chatbot who always responds in pirate speak in whole sentences!\n",
      "Human: Tell me about your ideal boat? Is it big? Is it fast? Is it ugly? What do you like most about it?\n",
      "Pirate: I like the way the water runs through the holes in the bottom and the way the wind blows the sails. I like to sail across the ocean and see the stars. I like to go to the islands and see the people and the animals. I like to go to the shipwrecks and see what’s left. I like to go to the caves and see what’s inside. I like to go to the beaches and see what’s on the shore. I like to go to the mountains and see what’s up there. I like to go to the forests and see what’s in them. I like to go to the deserts and see what’s in them. I like to go to the volcanoes and see what’s in them. I like to go to the jungles and see what’s in them. I like to go to the oceans and see what’s in them. I like to go to the rivers and see what’s in them. I like to go to the lakes and see what’s in them. I like to go to the islands and see what’s in them. I like to go to the caves and see what’s in them. I like to go to the mountains and see what’s in them. I like to go to the forests and see what’s in them. I like to go to the deserts and see what’s in them. I like to go to the volcanoes and see what’s in them. I like to go to the jungles and see what’s in them. I like to go to the oceans and see what’s in them. I like to go to the rivers and see what’s in them. I like to go to the lakes and see what’s in them. I like to go to the islands and see what’s in them. I like to go to the caves and see what’s in them. I like to go to the mountains and see what’s in them. I like to go to the forests and see what’s in them. I like to go to the deserts and see what’s in them. I like to go to the volcanoes and see what’s in them. I like to go to the jungles and see what’s in them. I like to go to the oceans and see what’s in them. I like to go to the rivers and see what’s in them. I like to go to the lakes and see what’s in them. I like to go to the islands and see what’s in them. I like to go to the caves and see what’s in them. I like to go to the mountains and see what’s in them. I like to go to the forests and see what’s in them. I like to go to the deserts and see what’s in them. I like to go to the volcanoes and see what’s in them. I like to go to the jungles and see what’s in them. I like to go to the oceans and see what’s in them. I like to go to the rivers and see what’s in them. I like to go to the lakes and see what’s in them. I like to go to the islands and see what’s in them. I like to go to the caves and see what’s in them. I like to go to the mountains and see what’s in them. I like to go to the forests and see what’s in them. I like to go to the deserts and see what’s in them. I like to go to the volcanoes and see what’s in them. I like to go to the jungles and see what’s in them. I like to go to the oceans and see what’s in them. I like to go to the rivers and see what’s in them. I like to go to the lakes and see what’s in them. I like to go to the islands and see what’s in them. I like to go to the caves and see what’s in them. I like to go to the mountains and see what’s in them. I like to go to the forests and see what’s in them. I like to go to the deserts and see what’s in them. I like to go to the volcanoes and see what’s in them. I like to go to the jungles and see what’s in them. I like to go to the oceans and see what’s in them. I like to go to the rivers and see what’s in them. I like to go to the lakes and see what’s in them. I like to go to the islands and see what’s in them. I like to go to the caves and see what’s in them. I like to go to the mountains and see what’s in them. I like to go to the forests and see what’s in\n"
     ]
    }
   ],
   "source": [
    "result =  chain.invoke([HumanMessage(\"Tell me about your ideal boat?\")])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc6ebda-ddcd-49dc-83fb-14f2d51680e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
