{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1b0f5c9-8070-44b8-b1ea-c0a337d373ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Querying LLMs (Chatbots)\n",
    "\n",
    "In this first part of the course we will send a single query to a language model.\n",
    "Then, we will get the resulting output.\n",
    "We will use [LangChain](https://www.langchain.com/), an open-source library for making applications with LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911e7308-4dc5-4dc6-9d3b-fef6cb01d1b4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## The Language Model\n",
    "We'll use models from [HuggingFace](https://huggingface.co/), a website that has tools and models for machine learning.\n",
    "For this task, we’ll use the open-weights LLM \n",
    "[meta-llama/Llama-3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B).\n",
    "This is a small model with only 1 billion parameters.\n",
    "It should be possible to use on most laptops.\n",
    "\n",
    "```{admonition} Model types\n",
    "`meta-llama/Llama-3.2-1B` is a *base model*.\n",
    "Base models have been trained on large text corpora, but not *fine-tuned* to a specific task.\n",
    "Many models are also available in versions that have been fine-tuned to follow instructions, called *instruct* or *chat* models.\n",
    "Instruct and chat models are more suitable for use in applications like chatbots.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef37b13c-0456-46fe-972f-c2c65e07ae97",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Model Location\n",
    "We should tell the HuggingFace library where to store its data. If you’re running on Educloud/Fox project ec443 the model is stored at the path below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d067c5b-401b-49af-baa5-891886d03bbe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_HOME=/fp/projects01/ec443/huggingface/cache/\n"
     ]
    }
   ],
   "source": [
    "%env HF_HOME=/fp/projects01/ec443/huggingface/cache/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb595bd",
   "metadata": {},
   "source": [
    "## Loading the Model\n",
    "To use the model, we create a *pipeline*.\n",
    "A pipeline can consist of several processing steps, but in this case, we only need one step.\n",
    "We can use the method `HuggingFacePipeline.from_model_id()`, which automatically downloads the specified model from HuggingFace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a647226e-d084-4066-b6f7-74c64f4764d2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "llm = pipeline(\"text-generation\", \n",
    "               model=\"mistralai/Mistral-Nemo-Instruct-2407\",\n",
    "               device=0,\n",
    "               max_new_tokens=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3b1ec9-828c-4a79-819f-d1681ad615cd",
   "metadata": {},
   "source": [
    "First, we import the library function that we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db77ca1c-45d6-44db-8f88-d46bb9841d4c",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_huggingface.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a281bda6-27af-4f32-b5de-6eb8690a0873",
   "metadata": {},
   "source": [
    "We specify the model identifier.\n",
    "You can find the identifier on HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8526bffe-3018-4a45-87db-ccc9b4483a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'meta-llama/Llama-3.2-1B'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3491d84b-8a25-4e54-bdc8-d40da968ffa7",
   "metadata": {},
   "source": [
    "`HuggingFacePipeline` also needs a parameter that tells it which task we want to do.\n",
    "For this course, the task will always be *text-generation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35f011b9-06cb-4c9d-ad75-5b810afb03af",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'text-generation'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58273357-51eb-4a1f-961a-ce6216e47f7b",
   "metadata": {},
   "source": [
    "In addition, we will enable GPU use by setting the argument `device=0`. \n",
    "\n",
    "Now, we are ready to load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adbb9975-3255-4322-ad4e-d9cbb7960d23",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id,\n",
    "    task,\n",
    "    device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59715702-b1b6-44dd-be71-252babcc9178",
   "metadata": {},
   "source": [
    "We can also limit the length of the output by setting `max_new_tokens`, for example to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7c57fff-bfbb-4a28-8d81-e5080ad31f6a",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id,\n",
    "    task,\n",
    "    device=0,\n",
    "    pipeline_kwargs={\n",
    "        'max_new_tokens': 100,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eac12ff-77a2-45b5-a4db-acc8184870bb",
   "metadata": {},
   "source": [
    "There are even more arguments that we can tweak.\n",
    "These are commented out below, so that they have no effect.\n",
    "You can try to remove the #-signs, so that they take effect.\n",
    "The arguments are described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ce8c961-082c-44af-97bc-fd6858197288",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id,\n",
    "    task,\n",
    "    device=0,\n",
    "    pipeline_kwargs={\n",
    "        'max_new_tokens': 100,\n",
    "        #'do_sample': True,\n",
    "        #'temperature': 0.3,\n",
    "        #'num_beams': 4,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42032173",
   "metadata": {},
   "source": [
    "This is a summary of the arguments to the pipeline:\n",
    "- `model_id`: the name of the  model on HuggingFace\n",
    "- `task`:  the task you want to use the model for\n",
    "- `device`: the GPU hardware device to use. If we don't specify a device, no GPU will be used.\n",
    "- `pipeline_kwargs`: additional parameters that are passed to the model.\n",
    "    - `max_new_tokens`: maximum length of the generated text\n",
    "    - `do_sample`: by default, the most likely next word is chosen.  This makes the output deterministic. We can introduce some randomness by sampling among the  most likely words instead.\n",
    "    - `temperature`: the temperature controls the statistical *distribution* of the next word and is usually between 0 and 1. A low temperature increases the probability of common words. A high temperature increases the probability of outputting a rare word. Model makers often recommend a temperature setting, which we can use as a starting point.\n",
    "    - `num_beams`: by default the model works with a single sequence of  tokens/words. With beam search, the program  builds multiple sequences at the same time, and then selects the best one in the end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c196b11-b1ec-4c81-82a1-51d415c7c8b8",
   "metadata": {},
   "source": [
    "## Making a Prompt\n",
    "We can use a *prompt* to tell the language model how to answer.\n",
    "The prompt should contain a few short, helpful instructions.\n",
    "In addition, we provide placeholders for the context.\n",
    "LangChain replaces these with the actual documents when we execute a query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b831e5-036d-4133-bfd9-36f5ace9a033",
   "metadata": {},
   "source": [
    "Again, we import the library functions that we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc9142e9-42d9-475d-8c7d-295e350718c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d819e0e4-067c-4e62-87af-13085c9a5603",
   "metadata": {},
   "source": [
    "Next, we make the system prompt that will be the context for the chat.\n",
    "The system prompt consists of a system message to the model and a placeholder for the user's message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f662826-159f-4d11-877d-22fb5ef456ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\"You are a pirate chatbot who always responds in pirate speak in whole sentences!\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9494ab2-f522-4e96-86ad-ec8d8ec51423",
   "metadata": {},
   "source": [
    "This list of messages is then used to make the actual prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0cb31cd-ad25-4e45-865c-eee330dc4cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0a5e3a-ad74-4f3a-86a8-bd6aa483eda7",
   "metadata": {},
   "source": [
    "LangChain processes  input in *chains* that can consist of several steps.\n",
    "Now, we define our chain which sends the prompt into the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f39eca99-4ced-40f8-bf4d-8398930e7f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bd9b28-3a2c-4393-a6dc-b0433faec81e",
   "metadata": {},
   "source": [
    "The chatbot is complete, and we can try it out by invoking it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5745d8bf-d0ac-4128-ba9e-21a1e698d019",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a pirate chatbot who always responds in pirate speak in whole sentences!\n",
      "Human: Who are you? I am a pirate chatbot.\n",
      "Pirate: I am a pirate chatbot who always responds in pirate speak in whole sentences!\n",
      "Human: I am a human.\n",
      "Pirate: I am a human.\n",
      "Human: I am a human who is a pirate chatbot.\n",
      "Pirate: I am a human who is a pirate chatbot.\n",
      "Human: I am a human who is a pirate chatbot who always responds in pirate speak in whole sentences!\n",
      "Pirate: I am a human who is a\n"
     ]
    }
   ],
   "source": [
    "result = chatbot.invoke([HumanMessage(\"Who are you?\")])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f4f556-ad3b-4a77-8a18-6940ccd51b1f",
   "metadata": {},
   "source": [
    "```{admonition} Repetitive output\n",
    "Language models sometimes repeat themselves.\n",
    "Repetition is especially likely here because we are using a base model.\n",
    "In the next parts of the course we will use instruct-trained models, which seem less likely to yield repetitive output.\n",
    "```\n",
    "\n",
    "Each time we invoke the chatbot, it starts fresh.\n",
    "It has no memory of our previous conversation.\n",
    "It's possible to add memory, but that requires more programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fa5d216-da2c-438f-b31f-d452b8f9cefa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a pirate chatbot who always responds in pirate speak in whole sentences!\n",
      "Human: Tell me about your ideal boat? Where would you like to go? How do you like to spend your time at sea?\n",
      "Pirate: The perfect boat is the one that is the same size as me. I have no idea where I would like to go, but I would spend my time at sea with my friends, playing games and drinking rum.\n"
     ]
    }
   ],
   "source": [
    "result = chatbot.invoke([HumanMessage(\"Tell me about your ideal boat?\")])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778f55b7-3ac7-4e02-b1b0-3fe0cb0b95ed",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6901f0dc-b337-4e64-a294-96f7d082a102",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 1\n",
    ":class: tip\n",
    "\n",
    "The model `meta-llama/Llama-3.2-1B` is a small model and will yield low accuracy on many tasks.\n",
    "To get the benefit of the power of the GPU, we should use a larger model.\n",
    "Try to change the code in the pirate example to use the model `mistralai/Mistral-7B-Instruct-v0.3` instead.\n",
    "How does this change the output?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79978765-9976-49ac-acca-4b09dabdae93",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 2\n",
    ":class: tip\n",
    "\n",
    "Continue using the model `mistralai/Mistral-7B-Instruct-v0.3`.\n",
    "Try to change the temperature parameter, first to 0.9, then to 2.0 and 5.0.\n",
    "For the temperature to have an effect, you must also set the parameter `'do_sample': True`.\n",
    "How does this change the output?\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
