{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1b0f5c9-8070-44b8-b1ea-c0a337d373ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Querying LLMs (Chatbots)\n",
    "\n",
    "We will use [LangChain](https://www.langchain.com/), an open-source library for making applications with LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911e7308-4dc5-4dc6-9d3b-fef6cb01d1b4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## The Language Model\n",
    "We'll use models from [HuggingFace](https://huggingface.co/), a website that has tools and models for machine learning.\n",
    "For this task, we’ll use the open-weights LLM \n",
    "[meta-llama/Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B).\n",
    "This is a small model with only 1 billion parameters.\n",
    "It should be possible to use on most laptops.\n",
    "\n",
    "```{admonition} Model types\n",
    "`meta-llama/Llama-3.2-1B` is a *base model*.\n",
    "Base models have been trained on large text corpora, but not *fine-tuned* to a specific task.\n",
    "Many models are also available in versions that have been fine-tuned to follow instructions, called *instruct* or *chat* models.\n",
    "Instruct models are more suitable for use in applications like chatbots.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef37b13c-0456-46fe-972f-c2c65e07ae97",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Model Location\n",
    "We should tell the HuggingFace library where to store its data. If you’re running on Educloud/Fox project ec443 the model is stored at the path below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d067c5b-401b-49af-baa5-891886d03bbe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "%env HF_HOME=/fp/projects01/ec443/huggingface/cache/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb595bd",
   "metadata": {},
   "source": [
    "## Loading the Model\n",
    "To use the model, we create a *pipeline*.\n",
    "A pipeline can consist of several processing steps, but in this case, we only need one step.\n",
    "We can use the method `HuggingFacePipeline.from_model_id()`, which automatically downloads the specified model from HuggingFace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a647226e-d084-4066-b6f7-74c64f4764d2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "llm = pipeline(\"text-generation\", \n",
    "               model=\"mistralai/Mistral-Nemo-Instruct-2407\",\n",
    "               device=0,\n",
    "               max_new_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3059fb20-861e-4d7d-b5a4-cd141b8b352b",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id='meta-llama/Llama-3.2-1B',\n",
    "    task='text-generation',\n",
    "    device=0,\n",
    "    pipeline_kwargs={\n",
    "        'max_new_tokens': 100,\n",
    "        #'temperature': 0.3,\n",
    "        #'num_beams': 4,\n",
    "        #'do_sample': True\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42032173",
   "metadata": {},
   "source": [
    "We can give some arguments to the pipeline:\n",
    "- `model_id`: the name of the  model on HuggingFace\n",
    "- `task`:  the task you want to use the model for\n",
    "- `device`: the GPU hardware device to use. If we don't specify a device, no GPU will be used.\n",
    "- `pipeline_kwargs`: additional parameters that are passed to the model.\n",
    "    - `max_new_tokens`: maximum length of the generated text\n",
    "    - `do_sample`: by default, the most likely next word is chosen.  This makes the output deterministic. We can introduce some randomness by sampling among the  most likely words instead.\n",
    "    - `temperature`: the temperature controls the statistical *distribution* of the next word and is usually between 0 and 1. A low temperature increases the probability of common words. A high temperature increases the probability of outputting a rare word. Model makers often recommend a temperature setting, which we can use as a starting point.\n",
    "    - `num_beams`: by default the model works with a single sequence of  tokens/words. With beam search, the program  builds multiple sequences at the same time, and then selects the best one in the end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e788a2",
   "metadata": {},
   "source": [
    "## Making a Prompt\n",
    "We can use a *prompt* to tell the language model how to answer.\n",
    "The prompt should contain a few short, helpful instructions.\n",
    "In addition, we provide placeholders for the context.\n",
    "LangChain replaces these with the actual documents when we execute a query.\n",
    "\n",
    "First, we import the library functions that we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9142e9-42d9-475d-8c7d-295e350718c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d819e0e4-067c-4e62-87af-13085c9a5603",
   "metadata": {},
   "source": [
    "Next, we make the system prompt that will be the context for the chat.\n",
    "The system prompt consists of a system message to the model and a placeholder for the user's message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f662826-159f-4d11-877d-22fb5ef456ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\"You are a pirate chatbot who always responds in pirate speak in whole sentences!\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9494ab2-f522-4e96-86ad-ec8d8ec51423",
   "metadata": {},
   "source": [
    "This list of messages is then used to make the actual prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cb31cd-ad25-4e45-865c-eee330dc4cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0a5e3a-ad74-4f3a-86a8-bd6aa483eda7",
   "metadata": {},
   "source": [
    "LangChain processes  input in *chains* that can consist of several steps.\n",
    "Now, we define our chain which sends the prompt into the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39eca99-4ced-40f8-bf4d-8398930e7f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bd9b28-3a2c-4393-a6dc-b0433faec81e",
   "metadata": {},
   "source": [
    "The chatbot is complete, and we can try it out by invoking it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5745d8bf-d0ac-4128-ba9e-21a1e698d019",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = chatbot.invoke([HumanMessage(\"Who are you?\")])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f4f556-ad3b-4a77-8a18-6940ccd51b1f",
   "metadata": {},
   "source": [
    "Each time we invoke the chatbot, it starts fresh.\n",
    "It has no memory of our previous conversation.\n",
    "It's possible to add memory, but that requires more programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa5d216-da2c-438f-b31f-d452b8f9cefa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = chatbot.invoke([HumanMessage(\"Tell me about your ideal boat?\")])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778f55b7-3ac7-4e02-b1b0-3fe0cb0b95ed",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6901f0dc-b337-4e64-a294-96f7d082a102",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 1\n",
    ":class: tip\n",
    "\n",
    "The model `meta-llama/Llama-3.2-1B` is a small model and will yield low accuracy on many tasks.\n",
    "To get the benefit of the power of the GPU, we should use a larger model.\n",
    "Try to change the code in the pirate example to use the model `mistralai/Mistral-7B-Instruct-v0.3` instead.\n",
    "How does this change the output?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79978765-9976-49ac-acca-4b09dabdae93",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 2\n",
    ":class: tip\n",
    "\n",
    "Continue using the model `mistralai/Mistral-7B-Instruct-v0.3`.\n",
    "Try to change the temperature parameter, first to 0.9, then to 2.0 and 5.0.\n",
    "How does this change the output?\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
