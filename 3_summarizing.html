
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Summarization &#8212; Introduction to Large Language Models on Fox</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '3_summarizing';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Retrieval-Augmented Generation" href="4_RAG.html" />
    <link rel="prev" title="Querying LLMs (Chatbots)" href="2_chatbot.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/uio_logo_small.svg" class="logo__image only-light" alt="Introduction to Large Language Models on Fox - Home"/>
    <script>document.write(`<img src="_static/uio_logo_small.svg" class="logo__image only-dark" alt="Introduction to Large Language Models on Fox - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="0_login.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_installing.html">Installing Software</a></li>
<li class="toctree-l1"><a class="reference internal" href="2_chatbot.html">Querying LLMs (Chatbots)</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Summarization</a></li>
<li class="toctree-l1"><a class="reference internal" href="4_RAG.html">Retrieval-Augmented Generation</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/uio-library/LLM-course/blob/master/3_summarizing.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/uio-library/LLM-course" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/uio-library/LLM-course/edit/master/3_summarizing.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/uio-library/LLM-course/issues/new?title=Issue%20on%20page%20%2F3_summarizing.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/3_summarizing.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Summarization</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#document-location">Document location</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-language-model">The Language Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#making-a-prompt">Making a Prompt</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#separating-the-summary-from-the-input">Separating the Summary from the Input</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-chain">Create chain</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-the-documents">Loading the Documents</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-the-summaries">Creating the Summaries</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#saving-the-summaries-to-text-files">Saving the Summaries to Text Files</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bonus-material">Bonus Material</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="summarization">
<h1>Summarization<a class="headerlink" href="#summarization" title="Link to this heading">#</a></h1>
<p>In this part of the course, we will attempt to use a language model to make summaries of some papers.
Making summaries of documents is also known as summarizing or summarization.
There exists specialized software for making summaries.
However, general large language models are also becoming quite good at this task.</p>
<p>Again, we will use <a class="reference external" href="https://www.langchain.com/">LangChain</a>, an open-source library for making applications with LLMs.</p>
<div class="tip admonition">
<p class="admonition-title">Exercise: Create new notebook</p>
<p>Create a new Jupyter Notebook called <code class="docutils literal notranslate"><span class="pre">summarizing</span></code> by clicking the <em>File</em>-menu in JupyterLab, and then <em>New</em> and <em>Notebook</em>.
If you are asked to select a kernel, choose <em>“Python 3”</em>.
Give the new notebook a name by clicking the <em>File</em>-menu in JupyterLab and then <em>Rename Notebook</em>.
Use the name <code class="docutils literal notranslate"><span class="pre">summarizing</span></code>.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Exercise: Stop old kernels</p>
<p>JupyterLab uses a Python <em>kernel</em> to execute the code in each notebook.
To free up GPU memory used in the previous chapter, you should stop the kernel for that notebook.
In the menu on the left side of JupyterLab, click the dark circle with a white square in it.
Then click <em>KERNELS</em> and <em>Shut Down All</em>.</p>
</div>
<section id="document-location">
<h2>Document location<a class="headerlink" href="#document-location" title="Link to this heading">#</a></h2>
<p>We have collected some papers licensed with a Creative Commons license.
We will try to load all the documents in the folder defined below.
If you prefer, you can change this to a different folder name.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">document_folder</span> <span class="o">=</span> <span class="s1">&#39;/fp/projects01/ec443/documents/terrorism&#39;</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="the-language-model">
<h2>The Language Model<a class="headerlink" href="#the-language-model" title="Link to this heading">#</a></h2>
<p>We’ll use models from <a class="reference external" href="https://huggingface.co/">HuggingFace</a>, a website that has tools and models for machine learning.
We’ll use the open-weights LLM
<a class="reference external" href="https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct">meta-llama/Llama-3.2-3B-Instruct</a>.
This model has a large context window, which means that we can use it to process quite large documents.
Yet it is small enough that we can use it with the smallest GPUs on Fox.
However, for better results you might want to use one of the somewhat larger models with around 7B or 8B parameters, for example
<a class="reference external" href="https://huggingface.co/mistralai/Ministral-8B-Instruct-2410">mistralai/Ministral-8B-Instruct-2410</a>.</p>
<div class="admonition-tokens-versus-words admonition">
<p class="admonition-title">Tokens versus Words</p>
<p>Short words can be a single token, but longer words usually consist of multiple tokens.
Therefore, the maximum document size with this model is less than 128k words.
Exactly how words are converted to tokens depends on the <em>tokenizer</em>.
LLMs usually come with tokenizers.
We will use the default tokenizer that ship with the LLM we use.</p>
</div>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;HF_HOME&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;/fp/projects01/ec443/huggingface/cache/&#39;</span>
</pre></div>
</div>
</div>
</div>
<p>To use the model, we create a <em>pipeline</em>.
A pipeline can consist of several processing steps, but in this case, we only need one step.
We can use the method <code class="docutils literal notranslate"><span class="pre">HuggingFacePipeline.from_model_id()</span></code>, which automatically downloads the specified model from HuggingFace.</p>
<p>As before, we check if we have a GPU available.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="n">device</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_community.llms</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFacePipeline</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">HuggingFacePipeline</span><span class="o">.</span><span class="n">from_model_id</span><span class="p">(</span>
    <span class="n">model_id</span><span class="o">=</span><span class="s1">&#39;meta-llama/Llama-3.2-3B-Instruct&#39;</span><span class="p">,</span>
    <span class="n">task</span><span class="o">=</span><span class="s1">&#39;text-generation&#39;</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
    <span class="n">pipeline_kwargs</span><span class="o">=</span><span class="p">{</span>
        <span class="s1">&#39;max_new_tokens&#39;</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="c1">#&#39;do_sample&#39;: True,</span>
        <span class="c1">#&#39;temperature&#39;: 0.3,</span>
        <span class="c1">#&#39;num_beams&#39;: 4,</span>
    <span class="p">}</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can give some arguments to the pipeline:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">model_id</span></code>: the name of the  model on HuggingFace</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">task</span></code>:  the task you want to use the model for</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">device</span></code>: the GPU hardware device to use. If we don’t specify a device, no GPU will be used.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pipeline_kwargs</span></code>: additional parameters that are passed to the model.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">max_new_tokens</span></code>: maximum length of the generated text</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">do_sample</span></code>: by default, the most likely next word is chosen.  This makes the output deterministic. We can introduce some randomness by sampling among the  most likely words instead.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">temperature</span></code>: the temperature controls the statistical <em>distribution</em> of the next word and is usually between 0 and 1. A low temperature increases the probability of common words. A high temperature increases the probability of outputting a rare word. Model makers often recommend a temperature setting, which we can use as a starting point.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_beams</span></code>: by default the model works with a single sequence of  tokens/words. With beam search, the program  builds multiple sequences at the same time, and then selects the best one in the end.</p></li>
</ul>
</li>
</ul>
</section>
<section id="making-a-prompt">
<h2>Making a Prompt<a class="headerlink" href="#making-a-prompt" title="Link to this heading">#</a></h2>
<p>We can use a <em>prompt</em> to tell the language model how to answer.
The prompt should contain a few short, helpful instructions.
In addition, we provide placeholders for the input,  called <em>context</em>.
LangChain replaces the placeholder with the input document when we execute a query.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.chains.combine_documents</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_stuff_documents_chain</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.chains.llm</span><span class="w"> </span><span class="kn">import</span> <span class="n">LLMChain</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.prompts</span><span class="w"> </span><span class="kn">import</span> <span class="n">PromptTemplate</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">separator</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Your Summary:</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="n">prompt_template</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;Write a summary of the following:</span>

<span class="si">{context}</span>
<span class="s1">&#39;&#39;&#39;</span> <span class="o">+</span> <span class="n">separator</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span><span class="n">template</span><span class="o">=</span><span class="n">prompt_template</span><span class="p">,</span>
                        <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;context&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="separating-the-summary-from-the-input">
<h2>Separating the Summary from the Input<a class="headerlink" href="#separating-the-summary-from-the-input" title="Link to this heading">#</a></h2>
<p>LangChain  returns both the input prompt and the generated response in one long text.
To get only the summary, we must split the summary from the document that we sent as input.
We can use the LangChain <em>output parser</em>
<a class="reference external" href="https://api.python.langchain.com/en/latest/langchain/output_parsers/langchain.output_parsers.regex.RegexParser.html">RegexParser</a> for this.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.output_parsers</span><span class="w"> </span><span class="kn">import</span> <span class="n">RegexParser</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">re</span>

<span class="n">output_parser</span> <span class="o">=</span> <span class="n">RegexParser</span><span class="p">(</span>
    <span class="n">regex</span><span class="o">=</span><span class="sa">rf</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">separator</span><span class="si">}</span><span class="s1">(.*)&#39;</span><span class="p">,</span>
    <span class="n">output_keys</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;summary&#39;</span><span class="p">],</span>
    <span class="n">flags</span><span class="o">=</span><span class="n">re</span><span class="o">.</span><span class="n">DOTALL</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="create-chain">
<h2>Create chain<a class="headerlink" href="#create-chain" title="Link to this heading">#</a></h2>
<p>The document loader loads each PDF page as a separate ‘document’.
This is partly for technical reasons because that is the way PDFs are structured.
Therefore, we use the chain called  <code class="docutils literal notranslate"><span class="pre">create_stuff_documents_chain</span></code> which joins multiple documents  into a single large document.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">chain</span> <span class="o">=</span> <span class="n">create_stuff_documents_chain</span><span class="p">(</span>
        <span class="n">llm</span><span class="p">,</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">output_parser</span><span class="o">=</span><span class="n">output_parser</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="loading-the-documents">
<h2>Loading the Documents<a class="headerlink" href="#loading-the-documents" title="Link to this heading">#</a></h2>
<p>We use LangChain’s <code class="docutils literal notranslate"><span class="pre">DirectoryLoader</span></code> to load all in files in <code class="docutils literal notranslate"><span class="pre">document_folder</span></code>.
<code class="docutils literal notranslate"><span class="pre">document_folder</span></code> is defined at the start of this  Notebook.</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_community.document_loaders</span><span class="w"> </span><span class="kn">import</span> <span class="n">DirectoryLoader</span>

<span class="n">loader</span> <span class="o">=</span> <span class="n">DirectoryLoader</span><span class="p">(</span><span class="n">document_folder</span><span class="p">)</span>
<span class="n">documents</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;number of documents:&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">documents</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="creating-the-summaries">
<h2>Creating the Summaries<a class="headerlink" href="#creating-the-summaries" title="Link to this heading">#</a></h2>
<p>Now, we can iterate over these documents with a <code class="docutils literal notranslate"><span class="pre">for</span></code>-loop.</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">summaries</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">:</span>
    <span class="n">filename</span> <span class="o">=</span> <span class="n">document</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="s1">&#39;source&#39;</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Summarizing document:&#39;</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">document</span><span class="p">]})</span>
    <span class="n">summary</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;summary&#39;</span><span class="p">]</span>
    <span class="n">summaries</span><span class="p">[</span><span class="n">filename</span><span class="p">]</span> <span class="o">=</span> <span class="n">summary</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Summary of file&#39;</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">summary</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="saving-the-summaries-to-text-files">
<h2>Saving the Summaries to Text Files<a class="headerlink" href="#saving-the-summaries-to-text-files" title="Link to this heading">#</a></h2>
<p>Finally, we save the summaries for later use.
We save all the summaries in the file <code class="docutils literal notranslate"><span class="pre">summaries.txt</span></code>.
If you like, you can store each summary in a separate file.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;summaries.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">outfile</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">summaries</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Summary of &#39;</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="n">file</span> <span class="o">=</span> <span class="n">outfile</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">summaries</span><span class="p">[</span><span class="n">filename</span><span class="p">],</span> <span class="n">file</span><span class="o">=</span><span class="n">outfile</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">file</span><span class="o">=</span><span class="n">outfile</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="bonus-material">
<h2>Bonus Material<a class="headerlink" href="#bonus-material" title="Link to this heading">#</a></h2>
<div class="tip dropdown admonition">
<p class="admonition-title">Make an Overall Summary</p>
<p>We can also try to generate an overall summary of all the documents.
This doesn’t make much sense with documents on different topics.
If all the documents are related or on the same topic, it could make sense to make an overall summary of all the summaries.</p>
<p>First, we need to import some more functions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.schema.document</span><span class="w"> </span><span class="kn">import</span> <span class="n">Document</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.prompts</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatPromptTemplate</span>
</pre></div>
</div>
<p>We make a new prompt, with more specific instructions than for the regular summaries.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">total_prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">(</span>
    <span class="p">[(</span><span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;Below is a list of summaries of some papers. Make a total summary all the information in all the papers:</span><span class="se">\n\n</span><span class="si">{context}</span><span class="se">\n\n</span><span class="s2">Total Summary:&quot;</span><span class="p">)]</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Then, we can make a new chain based on the LLM and the prompt:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">total_chain</span> <span class="o">=</span> <span class="n">create_stuff_documents_chain</span><span class="p">(</span><span class="n">llm</span><span class="p">,</span> <span class="n">total_prompt</span><span class="p">)</span>
</pre></div>
</div>
<p>This chain needs a list of  <code class="docutils literal notranslate"><span class="pre">Document</span></code> objects as input.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">list_of_summaries</span> <span class="o">=</span> <span class="p">[</span><span class="n">Document</span><span class="p">(</span><span class="n">summary</span><span class="p">)</span> <span class="k">for</span> <span class="n">summary</span> <span class="ow">in</span> <span class="n">summaries</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>
</pre></div>
</div>
<p>Now, we can invoke the chain with this list as input, and print the result:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">total_summary</span> <span class="o">=</span> <span class="n">total_chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="n">list_of_summaries</span><span class="p">})</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Summary of all the summaries:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">total_summary</span><span class="p">)</span>
</pre></div>
</div>
<p>Finally, we save the overall summary to a text file:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;total_summary.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">outfile</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">total_summary</span><span class="p">,</span> <span class="n">file</span><span class="o">=</span><span class="n">outfile</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h2>
<div class="tip admonition">
<p class="admonition-title">Exercise: Summarize your own document</p>
<p>Make a summary of a document that you upload to your own documents folder.
Read the summary carefully, and evaluate it with these questions in mind:</p>
<ul class="simple">
<li><p>Is the summary useful?</p></li>
<li><p>Is there anything missing from the summary?</p></li>
<li><p>Is the length of the summary suitable?</p></li>
</ul>
</div>
<div class="tip admonition">
<p class="admonition-title">Exercise: Adjust the summary</p>
<p>Try to make some adjustments to the prompt to modify the summary you got in exercise 1.
For example, you can ask for a longer or more concise summary.
Or you can tell the model to emphasize certain aspects of the text.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Exercise: Make a summary in a different language</p>
<p>We can use the model to get a summary in a different language from the original document.
For example, if the prompt is in Norwegian the response will usually also be Norwegian.
You can also specify on the prompt which language you want the summary to be in.
Use the model to make a summary of your document from exercise 1 in a different language.</p>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Bonus Exercise: Slurm Jobs</p>
<p>When you have made a program that works, it’s more efficient to run the program as a
<a class="reference external" href="https://www.uio.no/english/services/it/research/platforms/edu-research/help/fox/jobs/">batch job</a> than in JupyterLab.
This is because a JupyterLab session reserves a GPU all the time, also when you’re not running computations.
Therefore, you should save your finished program as a regular Python program that you can
<a class="reference external" href="https://training.pages.sigma2.no/tutorials/hpc-intro/episodes/13-scheduler.html">schedule</a> as a job.</p>
<p>You can save your code by clicking the “File”-menu in JupyterLab, click on “Save and Export Notebook As…” and then click “Executable Script”.
The result is the Python file <code class="docutils literal notranslate"><span class="pre">summarizing.py</span></code> that is downloaded to your local computer.
You will also need to download the slurm script
<a class="reference download internal" download="" href="_downloads/e57618b4370b86997b91d75aafb8a1ea/LLM.slurm"><code class="xref download docutils literal notranslate"><span class="pre">LLM.slurm</span></code></a>.</p>
<p>Upload both the Python file <code class="docutils literal notranslate"><span class="pre">summarizing.py</span></code> and the slurm script <code class="docutils literal notranslate"><span class="pre">LLM.slurm</span></code> to Fox.
Then, start the job with this command:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>! sbatch LLM.slurm summarizing.py
</pre></div>
</div>
<p>Slurm creates a log file for each job which is stored with a name like <code class="docutils literal notranslate"><span class="pre">slurm-1358473.out</span></code>.
By default, these log files are stored in the current working directory  where you run the <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> command.
If you want to store the log files somewhere else, you can add a line like below to your slurm script.
Remember to change the username.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>#SBATCH --output=/fp/projects01/ec443/&lt;username&gt;/logs/slurm-%j.out
</pre></div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="2_chatbot.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Querying LLMs (Chatbots)</p>
      </div>
    </a>
    <a class="right-next"
       href="4_RAG.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Retrieval-Augmented Generation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#document-location">Document location</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-language-model">The Language Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#making-a-prompt">Making a Prompt</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#separating-the-summary-from-the-input">Separating the Summary from the Input</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-chain">Create chain</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-the-documents">Loading the Documents</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-the-summaries">Creating the Summaries</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#saving-the-summaries-to-text-files">Saving the Summaries to Text Files</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bonus-material">Bonus Material</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Erik Winge
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>